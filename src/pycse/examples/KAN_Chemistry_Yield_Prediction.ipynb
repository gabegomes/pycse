{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KAN for Chemistry: Interpretable Yield Prediction\n",
    "\n",
    "This notebook demonstrates using Kolmogorov-Arnold Networks (KANs) for predicting reaction yields in the famous **Buchwald-Hartwig C-N cross-coupling** dataset from [Ahneman et al. (Science, 2018)](https://www.science.org/doi/10.1126/science.aar5169).\n",
    "\n",
    "## Why KANs for Chemistry?\n",
    "\n",
    "In chemistry, we often use **Multivariate Linear Regression (MLR)** because:\n",
    "1. Interpretability matters - we want to understand *why* a reaction works\n",
    "2. Datasets are often small (50-500 reactions)\n",
    "3. Physical meaning of parameters is important\n",
    "\n",
    "**KANs offer advantages over both MLR and black-box ML:**\n",
    "- Like MLR: Interpretable (can see what each input contributes)\n",
    "- Unlike MLR: Can capture nonlinear relationships\n",
    "- Unlike neural networks: Shows explicit activation functions on edges\n",
    "- The learned univariate functions may reveal physical relationships!\n",
    "\n",
    "## The Dataset\n",
    "\n",
    "The Buchwald-Hartwig dataset explores Pd-catalyzed C-N cross-coupling:\n",
    "- **4 Ligands**: Different phosphine ligands\n",
    "- **22 Additives**: Heterocyclic additives (potential catalyst poisons)\n",
    "- **3 Bases**: Different organic bases\n",
    "- **15 Aryl halides**: Different substrates\n",
    "- **Yield**: 0-100% (target variable)\n",
    "\n",
    "**References:**\n",
    "- Ahneman et al. \"Predicting reaction performance in C–N cross-coupling using machine learning\" *Science* 2018\n",
    "- [SigmanGroup](https://github.com/SigmanGroup) - Multivariate Linear Regression for catalysis\n",
    "- [Paton Lab](https://patonlab.com/) - Data-driven chemistry tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "# RDKit for molecular descriptors\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, AllChem\n",
    "\n",
    "# KAN from pycse\n",
    "import os\n",
    "os.environ[\"JAX_PLATFORM_NAME\"] = \"cpu\"\n",
    "from pycse.sklearn.kan import KAN\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Buchwald-Hartwig dataset\n",
    "# Download from: https://github.com/rxn4chemistry/rxn_yields\n",
    "\n",
    "# For this demo, we'll download directly\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "data_url = \"https://raw.githubusercontent.com/rxn4chemistry/rxn_yields/master/data/Buchwald-Hartwig/Dreher_and_Doyle_input_data.xlsx\"\n",
    "data_path = \"/tmp/buchwald_hartwig.xlsx\"\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    print(\"Downloading Buchwald-Hartwig dataset...\")\n",
    "    urllib.request.urlretrieve(data_url, data_path)\n",
    "    print(\"Done!\")\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_excel(data_path, sheet_name='Plates1-3')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nYield statistics:\")\n",
    "print(df['Output'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the yield distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(df['Output'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Yield (%)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Distribution of Reaction Yields')\n",
    "axes[0].axvline(df['Output'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"Output\"].mean():.1f}%')\n",
    "axes[0].legend()\n",
    "\n",
    "# Component counts\n",
    "components = ['Ligand', 'Additive', 'Base', 'Aryl halide']\n",
    "counts = [df[c].nunique() for c in components]\n",
    "axes[1].bar(components, counts, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
    "axes[1].set_ylabel('Number of Unique Components')\n",
    "axes[1].set_title('Reaction Components')\n",
    "for i, v in enumerate(counts):\n",
    "    axes[1].text(i, v + 0.5, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute Molecular Descriptors\n",
    "\n",
    "We use **RDKit** to compute molecular descriptors from SMILES strings.\n",
    "\n",
    "For each reaction component (ligand, additive, base, aryl halide), we calculate:\n",
    "- **MolWt**: Molecular weight\n",
    "- **MolLogP**: Lipophilicity (octanol-water partition coefficient)\n",
    "- **TPSA**: Topological polar surface area\n",
    "- **NumRotatableBonds**: Flexibility measure\n",
    "- **NumHAcceptors/Donors**: H-bonding capacity\n",
    "- **NumAromaticRings**: Aromaticity\n",
    "- **FractionCSP3**: Fraction sp3 carbons (3D character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mol_descriptors(smiles, prefix=''):\n",
    "    \"\"\"Calculate RDKit 2D descriptors for a molecule.\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    \n",
    "    # Selected descriptors relevant for reactivity\n",
    "    desc_funcs = {\n",
    "        'MolWt': Descriptors.MolWt,\n",
    "        'MolLogP': Descriptors.MolLogP,\n",
    "        'TPSA': Descriptors.TPSA,\n",
    "        'NumRotatableBonds': Descriptors.NumRotatableBonds,\n",
    "        'NumHAcceptors': Descriptors.NumHAcceptors,\n",
    "        'NumHDonors': Descriptors.NumHDonors,\n",
    "        'NumAromaticRings': Descriptors.NumAromaticRings,\n",
    "        'FractionCSP3': Descriptors.FractionCSP3,\n",
    "    }\n",
    "    \n",
    "    descriptors = {}\n",
    "    for name, func in desc_funcs.items():\n",
    "        try:\n",
    "            descriptors[f\"{prefix}{name}\"] = func(mol)\n",
    "        except:\n",
    "            descriptors[f\"{prefix}{name}\"] = np.nan\n",
    "    return descriptors\n",
    "\n",
    "# Compute descriptors for each component\n",
    "print(\"Computing molecular descriptors...\")\n",
    "\n",
    "# Create lookup tables for unique components\n",
    "component_cols = ['Ligand', 'Additive', 'Base', 'Aryl halide']\n",
    "prefixes = ['lig_', 'add_', 'base_', 'aryl_']\n",
    "\n",
    "desc_lookup = {}\n",
    "for col, prefix in zip(component_cols, prefixes):\n",
    "    unique_smiles = df[col].unique()\n",
    "    desc_lookup[col] = {}\n",
    "    for smi in unique_smiles:\n",
    "        desc = get_mol_descriptors(smi, prefix)\n",
    "        if desc is not None:\n",
    "            desc_lookup[col][smi] = desc\n",
    "    print(f\"  {col}: {len(desc_lookup[col])} molecules processed\")\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the feature matrix\n",
    "def build_feature_row(row):\n",
    "    \"\"\"Build feature vector for a reaction.\"\"\"\n",
    "    features = {}\n",
    "    for col, prefix in zip(component_cols, prefixes):\n",
    "        smi = row[col]\n",
    "        if smi in desc_lookup[col]:\n",
    "            features.update(desc_lookup[col][smi])\n",
    "        else:\n",
    "            # Handle missing\n",
    "            for name in ['MolWt', 'MolLogP', 'TPSA', 'NumRotatableBonds', \n",
    "                        'NumHAcceptors', 'NumHDonors', 'NumAromaticRings', 'FractionCSP3']:\n",
    "                features[f\"{prefix}{name}\"] = np.nan\n",
    "    return features\n",
    "\n",
    "# Apply to all rows\n",
    "feature_dicts = df.apply(build_feature_row, axis=1).tolist()\n",
    "X_df = pd.DataFrame(feature_dicts)\n",
    "y = df['Output'].values\n",
    "\n",
    "# Remove rows with NaN\n",
    "valid_mask = ~X_df.isna().any(axis=1)\n",
    "X_df = X_df[valid_mask]\n",
    "y = y[valid_mask]\n",
    "\n",
    "print(f\"Feature matrix shape: {X_df.shape}\")\n",
    "print(f\"Feature columns: {list(X_df.columns)}\")\n",
    "print(f\"\\nValid samples: {len(y)} / {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare Models: MLR vs Random Forest vs KAN\n",
    "\n",
    "We'll compare three approaches:\n",
    "1. **Multivariate Linear Regression (MLR)** - Traditional approach in physical organic chemistry\n",
    "2. **Random Forest** - What Ahneman et al. used (achieved R² ≈ 0.92 with DFT descriptors)\n",
    "3. **KAN** - Our interpretable nonlinear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X = X_df.values\n",
    "feature_names = list(X_df.columns)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "print(f\"Number of features: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate models\n",
    "results = {}\n",
    "\n",
    "# 1. Linear Regression (MLR)\n",
    "print(\"Training Multivariate Linear Regression...\")\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "results['MLR'] = {\n",
    "    'model': lr,\n",
    "    'r2': r2_score(y_test, y_pred_lr),\n",
    "    'mae': mean_absolute_error(y_test, y_pred_lr),\n",
    "    'y_pred': y_pred_lr\n",
    "}\n",
    "print(f\"  R² = {results['MLR']['r2']:.4f}, MAE = {results['MLR']['mae']:.2f}%\")\n",
    "\n",
    "# 2. Random Forest\n",
    "print(\"\\nTraining Random Forest...\")\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "results['Random Forest'] = {\n",
    "    'model': rf,\n",
    "    'r2': r2_score(y_test, y_pred_rf),\n",
    "    'mae': mean_absolute_error(y_test, y_pred_rf),\n",
    "    'y_pred': y_pred_rf\n",
    "}\n",
    "print(f\"  R² = {results['Random Forest']['r2']:.4f}, MAE = {results['Random Forest']['mae']:.2f}%\")\n",
    "\n",
    "# 3. KAN\n",
    "print(\"\\nTraining KAN...\")\n",
    "n_features = X_train.shape[1]\n",
    "kan = KAN(\n",
    "    layers=(n_features, 8, 1),  # Input -> 8 hidden -> 1 output\n",
    "    grid_size=5,\n",
    "    spline_order=3,\n",
    ")\n",
    "kan.fit(X_train, y_train, maxiter=300)\n",
    "y_pred_kan = kan.predict(X_test)\n",
    "results['KAN'] = {\n",
    "    'model': kan,\n",
    "    'r2': r2_score(y_test, y_pred_kan),\n",
    "    'mae': mean_absolute_error(y_test, y_pred_kan),\n",
    "    'y_pred': y_pred_kan\n",
    "}\n",
    "print(f\"  R² = {results['KAN']['r2']:.4f}, MAE = {results['KAN']['mae']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, (name, res) in zip(axes, results.items()):\n",
    "    ax.scatter(y_test, res['y_pred'], alpha=0.3, s=10)\n",
    "    ax.plot([0, 100], [0, 100], 'r--', linewidth=2, label='Perfect prediction')\n",
    "    ax.set_xlabel('Actual Yield (%)')\n",
    "    ax.set_ylabel('Predicted Yield (%)')\n",
    "    ax.set_title(f\"{name}\\nR² = {res['r2']:.3f}, MAE = {res['mae']:.1f}%\")\n",
    "    ax.set_xlim(-5, 105)\n",
    "    ax.set_ylim(-5, 105)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Comparison Summary\n",
    "\n",
    "Let's compare the models in terms of performance and interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Model':<20} {'R²':<10} {'MAE (%)':<10} {'Interpretable?'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "interpretability = {\n",
    "    'MLR': 'Yes (linear coefs)',\n",
    "    'Random Forest': 'Partial (feature importance)',\n",
    "    'KAN': 'Yes (activation functions)'\n",
    "}\n",
    "\n",
    "for name, res in results.items():\n",
    "    print(f\"{name:<20} {res['r2']:<10.4f} {res['mae']:<10.2f} {interpretability[name]}\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interpretability: Understanding the Models\n",
    "\n",
    "### 5.1 MLR: Linear Coefficients\n",
    "\n",
    "In MLR, each coefficient tells us how much the yield changes per unit change in that descriptor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLR coefficients\n",
    "lr_coefs = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': lr.coef_\n",
    "}).sort_values('Coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(\"Top 10 MLR Coefficients (most influential features):\")\n",
    "print(lr_coefs.head(10).to_string(index=False))\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = ['green' if c > 0 else 'red' for c in lr_coefs['Coefficient']]\n",
    "ax.barh(range(len(lr_coefs)), lr_coefs['Coefficient'], color=colors, alpha=0.7)\n",
    "ax.set_yticks(range(len(lr_coefs)))\n",
    "ax.set_yticklabels(lr_coefs['Feature'])\n",
    "ax.set_xlabel('Coefficient (effect on yield per std. dev.)')\n",
    "ax.set_title('MLR Coefficients: Linear Effects on Yield')\n",
    "ax.axvline(0, color='black', linewidth=0.5)\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Random Forest: Feature Importance\n",
    "\n",
    "Random Forest gives us feature importances, but no insight into *how* the feature affects the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest feature importance\n",
    "rf_importance = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': rf.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 Random Forest Feature Importances:\")\n",
    "print(rf_importance.head(10).to_string(index=False))\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.barh(range(len(rf_importance)), rf_importance['Importance'], color='steelblue', alpha=0.7)\n",
    "ax.set_yticks(range(len(rf_importance)))\n",
    "ax.set_yticklabels(rf_importance['Feature'])\n",
    "ax.set_xlabel('Feature Importance')\n",
    "ax.set_title('Random Forest: Feature Importances\\n(No insight into direction or shape of effect)')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 KAN: Learned Activation Functions\n",
    "\n",
    "**This is where KAN shines!** We can visualize the actual nonlinear transformations learned for each input.\n",
    "\n",
    "Unlike MLR (linear only) or Random Forest (black box), KAN shows us:\n",
    "- The **shape** of each input's effect\n",
    "- Whether the relationship is linear, quadratic, threshold-like, etc.\n",
    "- Potential **physical interpretations** of the learned functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize KAN activations for the input layer\n",
    "kan.plot_activations(layer_idx=0, figsize=(16, 12))\n",
    "plt.suptitle('KAN Input Layer Activations: How Each Descriptor Affects Yield', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network visualization\n",
    "kan.plot_network(figsize=(14, 10))\n",
    "plt.title('KAN Network Structure with Learned Activations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Chemical Interpretation\n",
    "\n",
    "Looking at the learned activation functions, we can extract chemical insights:\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "1. **Additive LogP**: Often shows nonlinear effects - too hydrophobic or too hydrophilic additives may poison the catalyst differently\n",
    "\n",
    "2. **Ligand Properties**: The phosphine ligand descriptors often show threshold-like behavior - the catalyst works well above a certain size/electron-donating capacity\n",
    "\n",
    "3. **Aryl Halide Reactivity**: Electronic properties (related to TPSA, H-bonding) can show optimal ranges\n",
    "\n",
    "### Comparison with Literature\n",
    "\n",
    "The Sigman group's MLR work has identified similar relationships but assumed linearity. KAN reveals where these assumptions break down!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial dependence-like analysis for top features\n",
    "# Show how yield changes with each descriptor while holding others constant\n",
    "\n",
    "def plot_partial_effect(model, feature_idx, X, y, feature_name, n_points=100):\n",
    "    \"\"\"Plot partial effect of a feature.\"\"\"\n",
    "    x_range = np.linspace(X[:, feature_idx].min(), X[:, feature_idx].max(), n_points)\n",
    "    X_temp = np.tile(X.mean(axis=0), (n_points, 1))\n",
    "    X_temp[:, feature_idx] = x_range\n",
    "    \n",
    "    y_pred = model.predict(X_temp)\n",
    "    return x_range, y_pred\n",
    "\n",
    "# Plot partial effects for top features\n",
    "top_features = rf_importance.head(6)['Feature'].tolist()\n",
    "top_indices = [feature_names.index(f) for f in top_features]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, feat, idx in zip(axes, top_features, top_indices):\n",
    "    # MLR prediction (linear)\n",
    "    x_range_lr, y_pred_lr = plot_partial_effect(lr, idx, X_scaled, y, feat)\n",
    "    \n",
    "    # KAN prediction (nonlinear)\n",
    "    x_range_kan, y_pred_kan = plot_partial_effect(kan, idx, X_scaled, y, feat)\n",
    "    \n",
    "    ax.plot(x_range_lr, y_pred_lr, 'b--', linewidth=2, label='MLR (linear)')\n",
    "    ax.plot(x_range_kan, y_pred_kan, 'r-', linewidth=2, label='KAN (nonlinear)')\n",
    "    ax.set_xlabel(f'{feat} (standardized)')\n",
    "    ax.set_ylabel('Predicted Yield (%)')\n",
    "    ax.set_title(feat)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Partial Effects: MLR vs KAN\\nKAN captures nonlinear relationships!', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Uncertainty Quantification with KAN\n",
    "\n",
    "KAN also supports uncertainty quantification through ensemble predictions - crucial for making confident predictions in chemistry!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train KAN with ensemble for uncertainty quantification\n",
    "print(\"Training KAN with uncertainty quantification...\")\n",
    "kan_uq = KAN(\n",
    "    layers=(n_features, 4, 1),\n",
    "    n_ensemble=32,  # Ensemble for UQ\n",
    "    grid_size=5,\n",
    "    loss_type='crps',\n",
    ")\n",
    "\n",
    "# Split validation set for calibration\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "kan_uq.fit(X_tr, y_tr, val_X=X_val, val_y=y_val, maxiter=200)\n",
    "\n",
    "# Get predictions with uncertainty\n",
    "y_pred_uq = kan_uq.predict(X_test)\n",
    "ensemble_preds = kan_uq.predict_ensemble(X_test)\n",
    "y_std = ensemble_preds.std(axis=1)\n",
    "\n",
    "print(f\"\\nTest R²: {r2_score(y_test, y_pred_uq):.4f}\")\n",
    "print(f\"Mean uncertainty (std): {y_std.mean():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions with uncertainty\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Prediction vs actual with error bars\n",
    "sort_idx = np.argsort(y_test)\n",
    "axes[0].errorbar(range(len(y_test)), y_pred_uq[sort_idx], yerr=2*y_std[sort_idx], \n",
    "                 fmt='none', alpha=0.3, capsize=0, color='red')\n",
    "axes[0].scatter(range(len(y_test)), y_test[sort_idx], s=10, alpha=0.5, label='Actual', color='blue')\n",
    "axes[0].scatter(range(len(y_test)), y_pred_uq[sort_idx], s=10, alpha=0.5, label='Predicted', color='red')\n",
    "axes[0].set_xlabel('Sample (sorted by actual yield)')\n",
    "axes[0].set_ylabel('Yield (%)')\n",
    "axes[0].set_title('Predictions with ±2σ Uncertainty')\n",
    "axes[0].legend()\n",
    "\n",
    "# Uncertainty vs error\n",
    "errors = np.abs(y_test - y_pred_uq)\n",
    "axes[1].scatter(y_std, errors, alpha=0.3, s=10)\n",
    "axes[1].plot([0, y_std.max()], [0, y_std.max()], 'r--', label='Error = σ')\n",
    "axes[1].set_xlabel('Predicted Uncertainty (σ)')\n",
    "axes[1].set_ylabel('Actual Error (|y - ŷ|)')\n",
    "axes[1].set_title('Uncertainty Calibration\\n(Well-calibrated: points near red line)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Conclusions\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **KAN bridges the gap** between simple interpretable models (MLR) and powerful but opaque models (Random Forest, Neural Networks)\n",
    "\n",
    "2. **Interpretability in chemistry**: KAN's learned activation functions can reveal:\n",
    "   - Nonlinear structure-activity relationships\n",
    "   - Optimal ranges for descriptors\n",
    "   - Threshold effects that linear models miss\n",
    "\n",
    "3. **Uncertainty quantification** is crucial for making actionable predictions in synthesis planning\n",
    "\n",
    "### When to Use KAN for Chemistry\n",
    "\n",
    "- When you want **interpretability** but suspect **nonlinear effects**\n",
    "- For small-to-medium datasets where neural networks might overfit\n",
    "- When **physical insight** into the learned relationships matters\n",
    "- As a complement to traditional Sigman-style MLR analysis\n",
    "\n",
    "### Future Directions\n",
    "\n",
    "- Use KAN with **DFT-computed descriptors** for higher accuracy\n",
    "- Apply to **enantioselectivity prediction** (Sigman's specialty)\n",
    "- Combine with **symbolic regression** to extract explicit equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final KAN report\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL KAN MODEL REPORT\")\n",
    "print(\"=\" * 60)\n",
    "kan.report()\n",
    "print(f\"\\nTest Performance:\")\n",
    "print(f\"  R² = {results['KAN']['r2']:.4f}\")\n",
    "print(f\"  MAE = {results['KAN']['mae']:.2f}%\")\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  MLR R² = {results['MLR']['r2']:.4f}\")\n",
    "print(f\"  Random Forest R² = {results['Random Forest']['r2']:.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
